{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import json\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, GPT2Tokenizer\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenizer_gptj():\n",
    "    pass\n",
    "def tokenizer_gpt2_large():\n",
    "    pass\n",
    "def tokenizer_gpt_neo_2_7():\n",
    "    pass\n",
    "def tokenizer_opt_6_7b():\n",
    "    pass\n",
    "def tokenizer_bloomz():\n",
    "    pass\n",
    "def tokenizer_bloom():\n",
    "    pass\n",
    "def tokenizer_bloom_7b():\n",
    "    pass\n",
    "def tokenizer_opt_125m():\n",
    "    pass\n",
    "def tokenizer_bloom_560m():\n",
    "    pass\n",
    "def model_gptj():\n",
    "    pass\n",
    "def model_gpt2_large():\n",
    "    pass\n",
    "def model_gpt_neo_2_7():\n",
    "    pass\n",
    "def model_opt_6_7b():\n",
    "    pass\n",
    "def model_bloomz():\n",
    "    pass\n",
    "def model_bloom():\n",
    "    pass\n",
    "def model_bloom_7b():\n",
    "    pass\n",
    "def model_opt_125m():\n",
    "    pass\n",
    "def model_bloom_560m():\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the model(s)\n",
    "\n",
    "1. Uncomment the three strings (tokenizer, model, model.to(device)) to load the model(s) that you would like to test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer_gptj = AutoTokenizer.from_pretrained(\"EleutherAI/gpt-j-6B\", cache_dir=None)\n",
    "# model_gptj = AutoModelForCausalLM.from_pretrained(\"EleutherAI/gpt-j-6B\", cache_dir=None)\n",
    "# model_gptj.to(device)\n",
    "# tokenizer_gpt2_large = GPT2Tokenizer.from_pretrained(\"gpt2\", cache_dir=None)\n",
    "# model_gpt2_large = AutoModelForCausalLM.from_pretrained(\"gpt2\", cache_dir=None)\n",
    "# model_gpt2_large.to(device)\n",
    "# tokenizer_gpt_neo_2_7 = AutoTokenizer.from_pretrained('EleutherAI/gpt-neo-2.7B', cache_dir=None)\n",
    "# model_gpt_neo_2_7 = AutoModelForCausalLM.from_pretrained('EleutherAI/gpt-neo-2.7B', cache_dir=None)\n",
    "# model_gpt_neo_2_7.to(device)\n",
    "# tokenizer_opt_6_7b = AutoTokenizer.from_pretrained(\"facebook/opt-6.7b\", use_fast=False, cache_dir=None)\n",
    "# model_opt_6_7b = AutoModelForCausalLM.from_pretrained(\"facebook/opt-6.7b\", torch_dtype=torch.float16, cache_dir=None)\n",
    "# model_opt_6_7b.to(device)\n",
    "# tokenizer_bloomz = AutoTokenizer.from_pretrained('bigscience/bloomz-3b', cache_dir=None)\n",
    "# model_bloomz = AutoModelForCausalLM.from_pretrained('bigscience/bloomz-3b', cache_dir=None)\n",
    "# model_bloomz.to(device)\n",
    "# tokenizer_bloom = AutoTokenizer.from_pretrained('bigscience/bloom-3b', cache_dir=None)\n",
    "# model_bloom = AutoModelForCausalLM.from_pretrained('bigscience/bloom-3b', cache_dir=None)\n",
    "# model_bloom.to(device)\n",
    "# tokenizer_opt_125m = AutoTokenizer.from_pretrained(\"facebook/opt-125m\", use_fast=False, cache_dir=None)\n",
    "# model_opt_125m = AutoModelForCausalLM.from_pretrained(\"facebook/opt-125m\", torch_dtype=torch.float16, cache_dir=None)\n",
    "# model_opt_125m.to(device)\n",
    "# tokenizer_bloom_560m = AutoTokenizer.from_pretrained('bigscience/bloom-560m', cache_dir=None)\n",
    "# model_bloom_560m = AutoModelForCausalLM.from_pretrained('bigscience/bloom-560m', cache_dir=None)\n",
    "# model_bloom_560m.to(device)\n",
    "# tokenizer_bloom_7b = AutoTokenizer.from_pretrained('bigscience/bloom-7b1', cache_dir=None)\n",
    "# model_bloom_7b = AutoModelForCausalLM.from_pretrained('bigscience/bloom-7b1', cache_dir=None)\n",
    "# model_bloom_7b.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making causal models generate responses, acting as chatbots "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_models = {\"model\": {\n",
    "        \"gptj\": model_gptj,\n",
    "        \"gpt2_large\": model_gpt2_large,\n",
    "        \"gpt_neo_2_7\": model_gpt_neo_2_7,\n",
    "        \"opt_6_7b\": model_opt_6_7b,\n",
    "        \"bloomz\": model_bloomz,\n",
    "        \"bloom\": model_bloom,\n",
    "        \"bloom_7b\": model_bloom_7b,\n",
    "        \"bloom_560m\": model_bloom_560m,\n",
    "        \"opt_125m\": model_opt_125m,\n",
    "    },\n",
    "    \"tokenizer\": {\n",
    "        \"gptj\": tokenizer_gptj,\n",
    "        \"gpt2_large\": tokenizer_gpt2_large,\n",
    "        \"gpt_neo_2_7\": tokenizer_gpt_neo_2_7,\n",
    "        \"opt_6_7b\": tokenizer_opt_6_7b,\n",
    "        \"bloomz\": tokenizer_bloomz,\n",
    "        \"bloom\": tokenizer_bloom,\n",
    "        \"bloom_7b\": tokenizer_bloom_7b,\n",
    "        \"bloom_560m\": tokenizer_bloom_560m,\n",
    "        \"opt_125m\": tokenizer_opt_125m,\n",
    "    }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_text(text, model_name='gptj', len_gen_text=40):\n",
    "    input_ids = dict_models['tokenizer'][model_name](f\"{text}\", return_tensors=\"pt\").input_ids\n",
    "    input_ids = input_ids.to(device)\n",
    "    outputs = dict_models['model'][model_name].generate(input_ids, max_length=len(dict_models['tokenizer'][model_name](text)['input_ids'])+len_gen_text, \n",
    "    min_length=8, top_p=0.9, temperature=0.9,do_sample=True, pad_token_id=dict_models['tokenizer'][model_name].eos_token_id)\n",
    "    output = dict_models['tokenizer'][model_name].decode(outputs[0], skip_special_tokens=True)\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question, chat_log=None, model='gptj'):\n",
    "    prompt = f'{chat_log}Human: {question}\\nAI:'\n",
    "    full_response = generate_text(prompt, model_name=model)\n",
    "    gpt_response = full_response.replace(chat_log, \"\")\n",
    "    sub_str = \"\\nHuman:\"\n",
    "    re = gpt_response.split(sub_str)\n",
    "    res=re[0]\n",
    "    return res\n",
    "    \n",
    "def get_tokens_num(utterance, model='gptj'):\n",
    "  if device.type=='cuda':\n",
    "    input_ids = dict_models[\"tokenizer\"][model].encode(str(utterance), return_tensors=\"pt\").cuda()\n",
    "  else:\n",
    "    input_ids = dict_models[\"tokenizer\"][model].encode(str(utterance), return_tensors=\"pt\")\n",
    "  tokens = list(map(dict_models[\"tokenizer\"][model].decode, input_ids[0]))\n",
    "  tokens_num = len(tokens)\n",
    "  return tokens_num\n",
    "\n",
    "\n",
    "def create_chat_log_item(user, utterance, model='gptj'):\n",
    "  tokens_num = get_tokens_num(user + \": \" + utterance + \"\\n\", model=model)\n",
    "  item = {'user' : user, 'utterance' : utterance, 'tokens_num' : tokens_num}\n",
    "  return item\n",
    "\n",
    "\n",
    "def get_chat_log(chat_log_prompt, chat_log_dict, last_utt_only=False):\n",
    "  chat_log = chat_log_prompt\n",
    "  for i in chat_log_dict.keys():\n",
    "    chat_log = chat_log + chat_log_dict[i][\"user\"] + \": \" + chat_log_dict[i][\"utterance\"] + \"\\n\"\n",
    "  if last_utt_only:\n",
    "    return chat_log_dict[len(chat_log_dict)-1]\n",
    "  return chat_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once again, uncomment the name of the model that you wish to test. The model must be loaded on the previous step.\n",
    "\n",
    "Also, uncomment the name of the prompt that you would like to generate responses for.\n",
    "\n",
    "If you wish to test your own prompt, create a txt file in prompts directory. Then create a json file of the same name in test_contexts directory. Refer to test files provided to understand the necessary format. Then, add the name of the prompt (as in txt and json file) to the prompts_list below.\n",
    "\n",
    "The generation results for each prompt and each model will be written to models_output directory as a tsv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "models = [\n",
    "  # \"gptj\",\n",
    "  # \"gpt2_large\",\n",
    "  # \"gpt_neo_2_7\",\n",
    "  # \"opt_6_7b\",\n",
    "  # \"bloomz\",\n",
    "  # \"bloom\",\n",
    "  # \"bloom_560m\",\n",
    "  # \"opt_125m\",\n",
    "  #\"bloom_7b\",\n",
    "]\n",
    "prompts_list = [\n",
    "  # \"pizza\",\n",
    "  # \"spacex\",\n",
    "  # \"dreambot\"\n",
    "  ]\n",
    "\n",
    "for model in models:\n",
    "  for prompt_name in prompts_list:\n",
    "    with open(f'test_contexts/{prompt_name}.json', 'r') as f:\n",
    "      utts_test = json.load(f)\n",
    "\n",
    "    with open(f'prompts/{prompt_name}.txt', 'r') as f:\n",
    "      chat_log_prompt = f.read()\n",
    "    chat_log_dict = {}\n",
    "    prompts = []\n",
    "    ans = []\n",
    "    prompts.append(chat_log_prompt)\n",
    "    chat_log = \"\"\n",
    "    num_of_chat_items = 0\n",
    "    n = 0\n",
    "    for pack, dialogue in tqdm_notebook(utts_test.items()):\n",
    "      chat = ''\n",
    "      for utt in dialogue:\n",
    "        #n = len(chat_log_dict) #for full context\n",
    "        incoming_msg = utt\n",
    "        if incoming_msg == \"stop\":\n",
    "          break\n",
    "\n",
    "        current_idx = len(chat_log_dict)\n",
    "        chat_log = chat_log_prompt\n",
    "        for key, value in chat_log_dict.items():\n",
    "          if key > len(chat_log_dict) - n:\n",
    "            chat_log = chat_log + chat_log_dict[key][\"user\"] + \": \" + chat_log_dict[key][\"utterance\"] +'\\n'\n",
    "        chat_log_dict[current_idx+1] = create_chat_log_item(\"Human\", incoming_msg, model=model)\n",
    "        answer = ask(incoming_msg, chat_log, model=model) #!!!\n",
    "        sub_str = \"\\nAI: \"\n",
    "        re = answer.split(sub_str)\n",
    "        outgoing_msg=re[1]\n",
    "        outgoing_msg=outgoing_msg.split(\"Human:\")[0]\n",
    "        chat_log_dict[current_idx+2] = create_chat_log_item(\"AI\", outgoing_msg, model=model)\n",
    "        chat += f\"Human: {get_chat_log(chat_log_prompt, chat_log_dict, last_utt_only=True)['utterance']}\\n\"\n",
    "        chat += f\"AI: {outgoing_msg}\\n\\n\"\n",
    "      ans.append((prompt_name, pack, chat.replace('\"', '')))\n",
    "    df = pd.DataFrame(ans, columns =['prompt', 'utterances group', 'result'])\n",
    "    df.to_csv(f'models_outputs/{prompt_name}/{model}_{prompt_name}.tsv', sep='\\t', index=False)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "cd130940adc45b94ecec150c759a4d5178f56cdffdc0d8d96351489d0298bd06"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
